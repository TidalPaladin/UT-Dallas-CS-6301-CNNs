{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 08 - Implementation\n",
    "### Scott Chase Waggener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requires Tensorflow 2.0 Alpha\n",
      "Tensorflow: 2.0.0-dev20190401\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('Requires Tensorflow 2.0 Alpha')\n",
    "print('Tensorflow: %s' % tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "We can use high level APIs in Tensorflow to build one of several well known models.\n",
    "Here we use MobileNetV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = tf.keras.Input(shape=(512, 1024, 3), batch_size=1)\n",
    "\n",
    "# Set weights=None or input_shape must match trained shape\n",
    "# Set include_top=False to generate a headless model\n",
    "model = tf.keras.applications.MobileNetV2(\n",
    "    input_tensor=input_tensor,\n",
    "    weights=None  \n",
    ")\n",
    "output_tensor = model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then iterate over the model layers to retrieve shape and type information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InputLayer             : [(1, 512, 1024, 3)] -> [(1, 512, 1024, 3)]\n",
      "ZeroPadding2D          : (1, 512, 1024, 3) -> (1, 513, 1025, 3)\n",
      "Conv2D                 : (1, 513, 1025, 3) -> (1, 256, 512, 32)\n",
      "BatchNormalization     : (1, 256, 512, 32) -> (1, 256, 512, 32)\n",
      "ReLU                   : (1, 256, 512, 32) -> (1, 256, 512, 32)\n",
      "DepthwiseConv2D        : (1, 256, 512, 32) -> (1, 256, 512, 32)\n",
      "BatchNormalization     : (1, 256, 512, 32) -> (1, 256, 512, 32)\n",
      "ReLU                   : (1, 256, 512, 32) -> (1, 256, 512, 32)\n",
      "Conv2D                 : (1, 256, 512, 32) -> (1, 256, 512, 16)\n",
      "BatchNormalization     : (1, 256, 512, 16) -> (1, 256, 512, 16)\n",
      "Conv2D                 : (1, 256, 512, 16) -> (1, 256, 512, 96)\n",
      "BatchNormalization     : (1, 256, 512, 96) -> (1, 256, 512, 96)\n",
      "ReLU                   : (1, 256, 512, 96) -> (1, 256, 512, 96)\n",
      "ZeroPadding2D          : (1, 256, 512, 96) -> (1, 257, 513, 96)\n",
      "DepthwiseConv2D        : (1, 257, 513, 96) -> (1, 128, 256, 96)\n",
      "BatchNormalization     : (1, 128, 256, 96) -> (1, 128, 256, 96)\n",
      "ReLU                   : (1, 128, 256, 96) -> (1, 128, 256, 96)\n",
      "Conv2D                 : (1, 128, 256, 96) -> (1, 128, 256, 24)\n",
      "BatchNormalization     : (1, 128, 256, 24) -> (1, 128, 256, 24)\n",
      "Conv2D                 : (1, 128, 256, 24) -> (1, 128, 256, 144)\n",
      "BatchNormalization     : (1, 128, 256, 144) -> (1, 128, 256, 144)\n",
      "ReLU                   : (1, 128, 256, 144) -> (1, 128, 256, 144)\n",
      "DepthwiseConv2D        : (1, 128, 256, 144) -> (1, 128, 256, 144)\n",
      "BatchNormalization     : (1, 128, 256, 144) -> (1, 128, 256, 144)\n",
      "ReLU                   : (1, 128, 256, 144) -> (1, 128, 256, 144)\n",
      "Conv2D                 : (1, 128, 256, 144) -> (1, 128, 256, 24)\n",
      "BatchNormalization     : (1, 128, 256, 24) -> (1, 128, 256, 24)\n",
      "Add                    : [(1, 128, 256, 24), (1, 128, 256, 24)] -> (1, 128, 256, 24)\n",
      "Conv2D                 : (1, 128, 256, 24) -> (1, 128, 256, 144)\n",
      "BatchNormalization     : (1, 128, 256, 144) -> (1, 128, 256, 144)\n",
      "ReLU                   : (1, 128, 256, 144) -> (1, 128, 256, 144)\n",
      "ZeroPadding2D          : (1, 128, 256, 144) -> (1, 129, 257, 144)\n",
      "DepthwiseConv2D        : (1, 129, 257, 144) -> (1, 64, 128, 144)\n",
      "BatchNormalization     : (1, 64, 128, 144) -> (1, 64, 128, 144)\n",
      "ReLU                   : (1, 64, 128, 144) -> (1, 64, 128, 144)\n",
      "Conv2D                 : (1, 64, 128, 144) -> (1, 64, 128, 32)\n",
      "BatchNormalization     : (1, 64, 128, 32) -> (1, 64, 128, 32)\n",
      "Conv2D                 : (1, 64, 128, 32) -> (1, 64, 128, 192)\n",
      "BatchNormalization     : (1, 64, 128, 192) -> (1, 64, 128, 192)\n",
      "ReLU                   : (1, 64, 128, 192) -> (1, 64, 128, 192)\n",
      "DepthwiseConv2D        : (1, 64, 128, 192) -> (1, 64, 128, 192)\n",
      "BatchNormalization     : (1, 64, 128, 192) -> (1, 64, 128, 192)\n",
      "ReLU                   : (1, 64, 128, 192) -> (1, 64, 128, 192)\n",
      "Conv2D                 : (1, 64, 128, 192) -> (1, 64, 128, 32)\n",
      "BatchNormalization     : (1, 64, 128, 32) -> (1, 64, 128, 32)\n",
      "Add                    : [(1, 64, 128, 32), (1, 64, 128, 32)] -> (1, 64, 128, 32)\n",
      "Conv2D                 : (1, 64, 128, 32) -> (1, 64, 128, 192)\n",
      "BatchNormalization     : (1, 64, 128, 192) -> (1, 64, 128, 192)\n",
      "ReLU                   : (1, 64, 128, 192) -> (1, 64, 128, 192)\n",
      "DepthwiseConv2D        : (1, 64, 128, 192) -> (1, 64, 128, 192)\n",
      "BatchNormalization     : (1, 64, 128, 192) -> (1, 64, 128, 192)\n",
      "ReLU                   : (1, 64, 128, 192) -> (1, 64, 128, 192)\n",
      "Conv2D                 : (1, 64, 128, 192) -> (1, 64, 128, 32)\n",
      "BatchNormalization     : (1, 64, 128, 32) -> (1, 64, 128, 32)\n",
      "Add                    : [(1, 64, 128, 32), (1, 64, 128, 32)] -> (1, 64, 128, 32)\n",
      "Conv2D                 : (1, 64, 128, 32) -> (1, 64, 128, 192)\n",
      "BatchNormalization     : (1, 64, 128, 192) -> (1, 64, 128, 192)\n",
      "ReLU                   : (1, 64, 128, 192) -> (1, 64, 128, 192)\n",
      "ZeroPadding2D          : (1, 64, 128, 192) -> (1, 65, 129, 192)\n",
      "DepthwiseConv2D        : (1, 65, 129, 192) -> (1, 32, 64, 192)\n",
      "BatchNormalization     : (1, 32, 64, 192) -> (1, 32, 64, 192)\n",
      "ReLU                   : (1, 32, 64, 192) -> (1, 32, 64, 192)\n",
      "Conv2D                 : (1, 32, 64, 192) -> (1, 32, 64, 64)\n",
      "BatchNormalization     : (1, 32, 64, 64) -> (1, 32, 64, 64)\n",
      "Conv2D                 : (1, 32, 64, 64) -> (1, 32, 64, 384)\n",
      "BatchNormalization     : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "ReLU                   : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "DepthwiseConv2D        : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "BatchNormalization     : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "ReLU                   : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "Conv2D                 : (1, 32, 64, 384) -> (1, 32, 64, 64)\n",
      "BatchNormalization     : (1, 32, 64, 64) -> (1, 32, 64, 64)\n",
      "Add                    : [(1, 32, 64, 64), (1, 32, 64, 64)] -> (1, 32, 64, 64)\n",
      "Conv2D                 : (1, 32, 64, 64) -> (1, 32, 64, 384)\n",
      "BatchNormalization     : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "ReLU                   : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "DepthwiseConv2D        : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "BatchNormalization     : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "ReLU                   : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "Conv2D                 : (1, 32, 64, 384) -> (1, 32, 64, 64)\n",
      "BatchNormalization     : (1, 32, 64, 64) -> (1, 32, 64, 64)\n",
      "Add                    : [(1, 32, 64, 64), (1, 32, 64, 64)] -> (1, 32, 64, 64)\n",
      "Conv2D                 : (1, 32, 64, 64) -> (1, 32, 64, 384)\n",
      "BatchNormalization     : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "ReLU                   : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "DepthwiseConv2D        : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "BatchNormalization     : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "ReLU                   : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "Conv2D                 : (1, 32, 64, 384) -> (1, 32, 64, 64)\n",
      "BatchNormalization     : (1, 32, 64, 64) -> (1, 32, 64, 64)\n",
      "Add                    : [(1, 32, 64, 64), (1, 32, 64, 64)] -> (1, 32, 64, 64)\n",
      "Conv2D                 : (1, 32, 64, 64) -> (1, 32, 64, 384)\n",
      "BatchNormalization     : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "ReLU                   : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "DepthwiseConv2D        : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "BatchNormalization     : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "ReLU                   : (1, 32, 64, 384) -> (1, 32, 64, 384)\n",
      "Conv2D                 : (1, 32, 64, 384) -> (1, 32, 64, 96)\n",
      "BatchNormalization     : (1, 32, 64, 96) -> (1, 32, 64, 96)\n",
      "Conv2D                 : (1, 32, 64, 96) -> (1, 32, 64, 576)\n",
      "BatchNormalization     : (1, 32, 64, 576) -> (1, 32, 64, 576)\n",
      "ReLU                   : (1, 32, 64, 576) -> (1, 32, 64, 576)\n",
      "DepthwiseConv2D        : (1, 32, 64, 576) -> (1, 32, 64, 576)\n",
      "BatchNormalization     : (1, 32, 64, 576) -> (1, 32, 64, 576)\n",
      "ReLU                   : (1, 32, 64, 576) -> (1, 32, 64, 576)\n",
      "Conv2D                 : (1, 32, 64, 576) -> (1, 32, 64, 96)\n",
      "BatchNormalization     : (1, 32, 64, 96) -> (1, 32, 64, 96)\n",
      "Add                    : [(1, 32, 64, 96), (1, 32, 64, 96)] -> (1, 32, 64, 96)\n",
      "Conv2D                 : (1, 32, 64, 96) -> (1, 32, 64, 576)\n",
      "BatchNormalization     : (1, 32, 64, 576) -> (1, 32, 64, 576)\n",
      "ReLU                   : (1, 32, 64, 576) -> (1, 32, 64, 576)\n",
      "DepthwiseConv2D        : (1, 32, 64, 576) -> (1, 32, 64, 576)\n",
      "BatchNormalization     : (1, 32, 64, 576) -> (1, 32, 64, 576)\n",
      "ReLU                   : (1, 32, 64, 576) -> (1, 32, 64, 576)\n",
      "Conv2D                 : (1, 32, 64, 576) -> (1, 32, 64, 96)\n",
      "BatchNormalization     : (1, 32, 64, 96) -> (1, 32, 64, 96)\n",
      "Add                    : [(1, 32, 64, 96), (1, 32, 64, 96)] -> (1, 32, 64, 96)\n",
      "Conv2D                 : (1, 32, 64, 96) -> (1, 32, 64, 576)\n",
      "BatchNormalization     : (1, 32, 64, 576) -> (1, 32, 64, 576)\n",
      "ReLU                   : (1, 32, 64, 576) -> (1, 32, 64, 576)\n",
      "ZeroPadding2D          : (1, 32, 64, 576) -> (1, 33, 65, 576)\n",
      "DepthwiseConv2D        : (1, 33, 65, 576) -> (1, 16, 32, 576)\n",
      "BatchNormalization     : (1, 16, 32, 576) -> (1, 16, 32, 576)\n",
      "ReLU                   : (1, 16, 32, 576) -> (1, 16, 32, 576)\n",
      "Conv2D                 : (1, 16, 32, 576) -> (1, 16, 32, 160)\n",
      "BatchNormalization     : (1, 16, 32, 160) -> (1, 16, 32, 160)\n",
      "Conv2D                 : (1, 16, 32, 160) -> (1, 16, 32, 960)\n",
      "BatchNormalization     : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "ReLU                   : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "DepthwiseConv2D        : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "BatchNormalization     : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "ReLU                   : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "Conv2D                 : (1, 16, 32, 960) -> (1, 16, 32, 160)\n",
      "BatchNormalization     : (1, 16, 32, 160) -> (1, 16, 32, 160)\n",
      "Add                    : [(1, 16, 32, 160), (1, 16, 32, 160)] -> (1, 16, 32, 160)\n",
      "Conv2D                 : (1, 16, 32, 160) -> (1, 16, 32, 960)\n",
      "BatchNormalization     : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "ReLU                   : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "DepthwiseConv2D        : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "BatchNormalization     : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "ReLU                   : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "Conv2D                 : (1, 16, 32, 960) -> (1, 16, 32, 160)\n",
      "BatchNormalization     : (1, 16, 32, 160) -> (1, 16, 32, 160)\n",
      "Add                    : [(1, 16, 32, 160), (1, 16, 32, 160)] -> (1, 16, 32, 160)\n",
      "Conv2D                 : (1, 16, 32, 160) -> (1, 16, 32, 960)\n",
      "BatchNormalization     : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "ReLU                   : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "DepthwiseConv2D        : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "BatchNormalization     : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "ReLU                   : (1, 16, 32, 960) -> (1, 16, 32, 960)\n",
      "Conv2D                 : (1, 16, 32, 960) -> (1, 16, 32, 320)\n",
      "BatchNormalization     : (1, 16, 32, 320) -> (1, 16, 32, 320)\n",
      "Conv2D                 : (1, 16, 32, 320) -> (1, 16, 32, 1280)\n",
      "BatchNormalization     : (1, 16, 32, 1280) -> (1, 16, 32, 1280)\n",
      "ReLU                   : (1, 16, 32, 1280) -> (1, 16, 32, 1280)\n",
      "GlobalAveragePooling2D : (1, 16, 32, 1280) -> (1, 1280)      \n",
      "Dense                  :       (1, 1280) -> (1, 1000)      \n"
     ]
    }
   ],
   "source": [
    "FMT = \"%-22s : %15s -> %-15s\"\n",
    "for layer in model.layers:\n",
    "    name = type(layer).__name__\n",
    "    inp= layer.input_shape \n",
    "    out= layer.output_shape\n",
    "    msg = FMT % (name, inp, out)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes us through the graph portion of the problem. \n",
    "\n",
    "### Hardware Parameters\n",
    "\n",
    "Here we add the given hardware parameters directly as a `dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flags():\n",
    "    def __init__(self, init):\n",
    "        for k, v in init.items():\n",
    "            if type(v) is dict and type(next(iter(v.keys()))) is not int:\n",
    "                init[k] = Flags(v)\n",
    "        self.__dict__ = init\n",
    "\n",
    "hw = Flags({\n",
    "    'ddr': {\n",
    "        'bits': 64,\n",
    "        'freq': 3200e6,\n",
    "        'avail': 0.5,\n",
    "        'eff': 0.8\n",
    "    },\n",
    "    'mem': {\n",
    "        'off': 1e10,\n",
    "        'on': 4e6\n",
    "    },\n",
    "    'comp': {\n",
    "        'freq': 1e9,\n",
    "        'mat': {\n",
    "            8: (32, 32, 32),\n",
    "            16: (32, 16, 16),\n",
    "            32: (32, 8, 8)\n",
    "        },\n",
    "        'ticks_per_tile': 32,\n",
    "        'vec': {\n",
    "            8: 32,\n",
    "            16: 16,\n",
    "            32: 8\n",
    "        },\n",
    "        'ticks_per_vec': 1\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "Next we need functions to compute the various required metrics. Specifically,\n",
    "we need functions that map Keras layers to their corresponding serial \n",
    "and parallel times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcode\n",
    "precision = 16\n",
    "mat_tile = hw.comp.mat[precision]\n",
    "vec_tile = hw.comp.vec[precision]\n",
    "\n",
    "def time(layer) -> float:\n",
    "    name =type(layer).__name__\n",
    "    if name in ['Conv2D', 'DepthwiseConv2D', 'Dense']:\n",
    "        return matrix_time(layer)\n",
    "    else:\n",
    "        return vector_time(layer)\n",
    "\n",
    "\n",
    "def matrix_time(layer) -> (float, float):\n",
    "    name = type(layer).__name__\n",
    "\n",
    "    # Set Fr, Fc for depthwise vs regular\n",
    "    if name is 'Conv2D':\n",
    "        Fr, Fc = (3, 3)\n",
    "    else:\n",
    "        Fr, Fc = (1, 1)\n",
    "\n",
    "    # Set convolution dimensions\n",
    "    Lr, Lc = layer.output_shape[1], layer.output_shape[2]\n",
    "    No, Ni = layer.output_shape[-1], layer.input_shape[-1]\n",
    "    M_conv = No \n",
    "    K_conv = Ni * Fr * Fc\n",
    "    N_conv = Lr * Lc\n",
    "\n",
    "    # Tile dimensions of matrix matrix multiply primitive\n",
    "    M, N, K = mat_tile\n",
    "\n",
    "    # Calculate required tiles as ceil of conv2d dims / primitive dims\n",
    "    tiles = [\n",
    "            (c // m + 1) \n",
    "            for c, m in zip([M_conv, N_conv, K_conv], [M, N, K])\n",
    "    ]\n",
    "\n",
    "    # Calculate compute time\n",
    "    tile_product = 1\n",
    "    for tile_count in tiles:\n",
    "        tile_product *= tile_count \n",
    "    return tile_product * hw.comp.ticks_per_tile / hw.comp.freq * 1e6\n",
    "\n",
    "\n",
    "def data_time(layer) -> float:\n",
    "\n",
    "    # Weights all marked off device\n",
    "    weight_time = precision / 8 * layer.count_params() / hw.ddr.freq\n",
    "\n",
    "    if name in ['Add']:\n",
    "        feature_size = precision / 8 * layer.input[0].shape.num_elements()\n",
    "    elif name in ['InputLayer']:\n",
    "        feature_size = precision / 8 * tf.TensorShape(layer.input.shape).num_elements()\n",
    "    else:\n",
    "        feature_size = precision / 8 * layer.input.shape.num_elements()\n",
    "\n",
    "    if feature_size > hw.mem.on or name is 'InputLayer':\n",
    "        feature_time = feature_size / hw.ddr.freq\n",
    "    else:\n",
    "        feature_time = 0\n",
    "\n",
    "    return (weight_time + feature_time) * 1e6\n",
    "\n",
    "\n",
    "def vector_time(layer) -> (float, float):\n",
    "\n",
    "    # Add gives two input shapes so pull out one\n",
    "    if name is 'Add':\n",
    "        shape = layer.input_shape[0][1:]\n",
    "    else:\n",
    "        shape = layer.input_shape\n",
    "\n",
    "    N = shape[0]\n",
    "    return (N // vec_tile + 1) * hw.comp.ticks_per_tile / hw.comp.freq * 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "We can then loop over the layers and extract the relevant times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer                  : Comp(us) / Data(us)\n",
      "============================================\n",
      "InputLayer             : 0.00     /   983.04\n",
      "ZeroPadding2D          : 0.03     /     0.00\n",
      "Conv2D                 : 1048.70  /     0.54\n",
      "BatchNormalization     : 0.03     /  2621.52\n",
      "ReLU                   : 0.03     /  2621.44\n",
      "DepthwiseConv2D        : 1573.06  /  2621.62\n",
      "BatchNormalization     : 0.03     /  2621.52\n",
      "ReLU                   : 0.03     /  2621.44\n",
      "Conv2D                 : 4981.34  /  2621.76\n",
      "BatchNormalization     : 0.03     /  1310.76\n",
      "Conv2D                 : 10487.04 /  1311.68\n",
      "BatchNormalization     : 0.03     /  7864.56\n",
      "ReLU                   : 0.03     /  7864.32\n",
      "ZeroPadding2D          : 0.03     /  7864.32\n",
      "DepthwiseConv2D        : 1835.90  /  7911.00\n",
      "BatchNormalization     : 0.03     /  1966.32\n",
      "ReLU                   : 0.03     /  1966.08\n",
      "Conv2D                 : 3606.24  /  1967.52\n",
      "BatchNormalization     : 0.03     /     0.06\n",
      "Conv2D                 : 4589.76  /     2.16\n",
      "BatchNormalization     : 0.03     /  2949.48\n",
      "ReLU                   : 0.03     /  2949.12\n",
      "DepthwiseConv2D        : 3278.40  /  2949.93\n",
      "BatchNormalization     : 0.03     /  2949.48\n",
      "ReLU                   : 0.03     /  2949.12\n",
      "Conv2D                 : 5376.58  /  2951.28\n",
      "BatchNormalization     : 0.03     /     0.06\n",
      "Add                    : 0.29     /     0.00\n",
      "Conv2D                 : 4589.76  /     2.16\n",
      "BatchNormalization     : 0.03     /  2949.48\n",
      "ReLU                   : 0.03     /  2949.12\n",
      "ZeroPadding2D          : 0.03     /  2949.12\n",
      "DepthwiseConv2D        : 820.80   /  2984.58\n",
      "BatchNormalization     : 0.03     /     0.36\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 2692.22  /     2.88\n",
      "BatchNormalization     : 0.03     /     0.08\n",
      "Conv2D                 : 2183.33  /     3.84\n",
      "BatchNormalization     : 0.03     /     0.48\n",
      "ReLU                   : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 1493.86  /     1.08\n",
      "BatchNormalization     : 0.03     /     0.48\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 3578.69  /     3.84\n",
      "BatchNormalization     : 0.03     /     0.08\n",
      "Add                    : 0.16     /     0.00\n",
      "Conv2D                 : 2183.33  /     3.84\n",
      "BatchNormalization     : 0.03     /     0.48\n",
      "ReLU                   : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 1493.86  /     1.08\n",
      "BatchNormalization     : 0.03     /     0.48\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 3578.69  /     3.84\n",
      "BatchNormalization     : 0.03     /     0.08\n",
      "Add                    : 0.16     /     0.00\n",
      "Conv2D                 : 2183.33  /     3.84\n",
      "BatchNormalization     : 0.03     /     0.48\n",
      "ReLU                   : 0.03     /     0.00\n",
      "ZeroPadding2D          : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 375.65   /     1.08\n",
      "BatchNormalization     : 0.03     /     0.48\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 1349.86  /     7.68\n",
      "BatchNormalization     : 0.03     /     0.16\n",
      "Conv2D                 : 1985.57  /    15.36\n",
      "BatchNormalization     : 0.03     /     0.96\n",
      "ReLU                   : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 1341.60  /     2.16\n",
      "BatchNormalization     : 0.03     /     0.96\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 2687.33  /    15.36\n",
      "BatchNormalization     : 0.03     /     0.16\n",
      "Add                    : 0.10     /     0.00\n",
      "Conv2D                 : 1985.57  /    15.36\n",
      "BatchNormalization     : 0.03     /     0.96\n",
      "ReLU                   : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 1341.60  /     2.16\n",
      "BatchNormalization     : 0.03     /     0.96\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 2687.33  /    15.36\n",
      "BatchNormalization     : 0.03     /     0.16\n",
      "Add                    : 0.10     /     0.00\n",
      "Conv2D                 : 1985.57  /    15.36\n",
      "BatchNormalization     : 0.03     /     0.96\n",
      "ReLU                   : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 1341.60  /     2.16\n",
      "BatchNormalization     : 0.03     /     0.96\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 2687.33  /    15.36\n",
      "BatchNormalization     : 0.03     /     0.16\n",
      "Add                    : 0.10     /     0.00\n",
      "Conv2D                 : 1985.57  /    15.36\n",
      "BatchNormalization     : 0.03     /     0.96\n",
      "ReLU                   : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 1341.60  /     2.16\n",
      "BatchNormalization     : 0.03     /     0.96\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 3583.10  /    23.04\n",
      "BatchNormalization     : 0.03     /     0.24\n",
      "Conv2D                 : 4313.76  /    34.56\n",
      "BatchNormalization     : 0.03     /     1.44\n",
      "ReLU                   : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 2901.98  /     3.24\n",
      "BatchNormalization     : 0.03     /     1.44\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 5366.40  /    34.56\n",
      "BatchNormalization     : 0.03     /     0.24\n",
      "Add                    : 0.10     /     0.00\n",
      "Conv2D                 : 4313.76  /    34.56\n",
      "BatchNormalization     : 0.03     /     1.44\n",
      "ReLU                   : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 2901.98  /     3.24\n",
      "BatchNormalization     : 0.03     /     1.44\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 5366.40  /    34.56\n",
      "BatchNormalization     : 0.03     /     0.24\n",
      "Add                    : 0.10     /     0.00\n",
      "Conv2D                 : 4313.76  /    34.56\n",
      "BatchNormalization     : 0.03     /     1.44\n",
      "ReLU                   : 0.03     /     0.00\n",
      "ZeroPadding2D          : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 742.37   /     3.24\n",
      "BatchNormalization     : 0.03     /     1.44\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 2059.20  /    57.60\n",
      "BatchNormalization     : 0.03     /     0.40\n",
      "Conv2D                 : 2978.98  /    96.00\n",
      "BatchNormalization     : 0.03     /     2.40\n",
      "ReLU                   : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 1996.90  /     5.40\n",
      "BatchNormalization     : 0.03     /     2.40\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 3427.78  /    96.00\n",
      "BatchNormalization     : 0.03     /     0.40\n",
      "Add                    : 0.06     /     0.00\n",
      "Conv2D                 : 2978.98  /    96.00\n",
      "BatchNormalization     : 0.03     /     2.40\n",
      "ReLU                   : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 1996.90  /     5.40\n",
      "BatchNormalization     : 0.03     /     2.40\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 3427.78  /    96.00\n",
      "BatchNormalization     : 0.03     /     0.40\n",
      "Add                    : 0.06     /     0.00\n",
      "Conv2D                 : 2978.98  /    96.00\n",
      "BatchNormalization     : 0.03     /     2.40\n",
      "ReLU                   : 0.03     /     0.00\n",
      "DepthwiseConv2D        : 1996.90  /     5.40\n",
      "BatchNormalization     : 0.03     /     2.40\n",
      "ReLU                   : 0.03     /     0.00\n",
      "Conv2D                 : 6284.26  /   192.00\n",
      "BatchNormalization     : 0.03     /     0.80\n",
      "Conv2D                 : 7836.58  /   256.00\n",
      "BatchNormalization     : 0.03     /     3.20\n",
      "ReLU                   : 0.03     /     0.00\n",
      "GlobalAveragePooling2D : 0.03     /     0.00\n",
      "Dense                  : 0.03     /   800.62\n",
      "============================================\n",
      "Total                  : 156441.98 / 88472.49\n"
     ]
    }
   ],
   "source": [
    "HEAD = \"%-22s : %7s / %7s\" % ('Layer', 'Comp(us)', 'Data(us)')\n",
    "print(HEAD)\n",
    "print('=' * len(HEAD))\n",
    "\n",
    "FMT = \"%-22s : %-8.2f / %8.2f\"\n",
    "comp_total = 0\n",
    "data_total = 0\n",
    "for layer in model.layers: #1: skips input layer\n",
    "    name = type(layer).__name__\n",
    "\n",
    "    if name in ['Conv2D', 'DepthwiseConv2D']:\n",
    "        comp = matrix_time(layer)\n",
    "    elif name in ['InputLayer']:\n",
    "        comp = 0\n",
    "    else:\n",
    "        comp = vector_time(layer)\n",
    "\n",
    "    data = data_time(layer)\n",
    "    msg = FMT % (name, comp, data)\n",
    "    print(msg)\n",
    "\n",
    "    comp_total += comp\n",
    "    data_total += data\n",
    "\n",
    "print('=' * len(HEAD))\n",
    "print(FMT % ('Total', comp_total, data_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can the trivially compute bounds in the serial and parallel case by considering\n",
    "the sum of times for serial operations and the max of times for parallel operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
